{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 0 0 3 \n",
      "States: 0 0 0 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 0 0 0 0 0 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 ...\n",
      "States: 0 0 0 0 0 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 0 0 0 0 3 \n",
      "States: 0 3 \n",
      "States: 0 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 ...\n",
      "States: 0 0 0 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n"
     ]
    }
   ],
   "source": [
    "transition_probabilities = [\n",
    "        [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n",
    "        [0.0, 0.0, 0.9, 0.1],  # from s1 to ...\n",
    "        [0.0, 1.0, 0.0, 0.0],  # from s2 to ...\n",
    "        [0.0, 0.0, 0.0, 1.0],  # from s3 to ...\n",
    "    ]\n",
    "\n",
    "n_max_steps = 50\n",
    "\n",
    "def print_sequence(start_state=0):\n",
    "    current_state = start_state\n",
    "    print(\"States:\", end=\" \")\n",
    "    for step in range(n_max_steps):\n",
    "        print(current_state, end=\" \")\n",
    "        if current_state == 3:\n",
    "            break\n",
    "        current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
    "    else:\n",
    "        print(\"...\", end=\"\")\n",
    "    print()\n",
    "\n",
    "for _ in range(10):\n",
    "    print_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "Markov decision processes are an extension of Markov chains;\n",
    "the difference is the addition of actions (allowing choice) and rewards (giving motivation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nan=np.nan # represents impossible actions\n",
    "\n",
    "#transition_probabilities\n",
    "T = np.array([ # in s0, if action a0 then proba 0.7 to state s0 and 0.3 to state s1, etc.\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "        [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],\n",
    "        [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]],\n",
    "    ])\n",
    "# rewards\n",
    "R = np.array([ \n",
    "        [[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "        [[10., 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50.]],\n",
    "        [[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]],\n",
    "    ])\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_fire\n",
      "States (+rewards): 0 (10.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 1 (-50.0) 2 (40.0) 0 1 (-50.0) ... Total rewards = 260.0\n",
      "States (+rewards): 0 (10.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 1 (-50.0) 2 (40.0) 0 1 (-50.0) 2 (40.0) ... Total rewards = 290.0\n",
      "States (+rewards): 0 1 (-50.0) 2 (40.0) 0 1 (-50.0) 2 (40.0) 0 1 (-50.0) 2 (40.0) 0 ... Total rewards = 210.0\n",
      "States (+rewards): 0 (10.0) 0 1 (-50.0) 2 2 (40.0) 0 (10.0) 0 (10.0) 0 1 (-50.0) 2 (40.0) ... Total rewards = 110.0\n",
      "States (+rewards): 0 (10.0) 0 (10.0) 0 1 (-50.0) 2 2 (40.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 ... Total rewards = 150.0\n",
      "Summary: mean=127.6, std=129.863953, min=-390.0, max=520.0\n",
      "\n",
      "policy_random\n",
      "States (+rewards): 0 0 (10.0) 0 0 0 (10.0) 0 (10.0) 0 1 1 1 (-50.0) ... Total rewards = 160.0\n",
      "States (+rewards): 0 (10.0) 0 (10.0) 0 0 (10.0) 0 0 (10.0) 0 0 1 (-50.0) 2 (40.0) ... Total rewards = -70.0\n",
      "States (+rewards): 0 (10.0) 0 1 1 (-50.0) 2 (40.0) 0 0 1 1 1 (-50.0) ... Total rewards = 70.0\n",
      "States (+rewards): 0 (10.0) 0 (10.0) 0 0 0 0 0 0 (10.0) 0 0 ... Total rewards = 0.0\n",
      "States (+rewards): 0 0 1 (-50.0) 2 (40.0) 0 (10.0) 0 1 (-50.0) 2 (40.0) 0 1 (-50.0) ... Total rewards = 60.0\n",
      "Summary: mean=-24.4, std=87.991270, min=-330.0, max=250.0\n",
      "\n",
      "policy_safe\n",
      "States (+rewards): 0 (10.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 1 1 1 1 1 ... Total rewards = 40.0\n",
      "States (+rewards): 0 (10.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 1 1 1 1 1 ... Total rewards = 40.0\n",
      "States (+rewards): 0 (10.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 1 1 ... Total rewards = 70.0\n",
      "States (+rewards): 0 (10.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 (10.0) 0 1 1 1 1 ... Total rewards = 50.0\n",
      "States (+rewards): 0 1 1 1 1 1 1 1 1 1 ... Total rewards = 0.0\n",
      "Summary: mean=22.3, std=27.021234, min=0.0, max=330.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def policy_fire(state):\n",
    "    return [0, 2, 1][state]\n",
    "\n",
    "def policy_random(state):\n",
    "    return np.random.choice(possible_actions[state])\n",
    "\n",
    "def policy_safe(state):\n",
    "    return [0, 0, 1][state]\n",
    "\n",
    "class MDPEnvironment(object):\n",
    "    def __init__(self, start_state=0):\n",
    "        self.start_state=start_state\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.total_rewards = 0\n",
    "        self.state = self.start_state\n",
    "    def step(self, action):\n",
    "        next_state = np.random.choice(range(3), p=T[self.state][action])\n",
    "        reward = R[self.state][action][next_state]\n",
    "        self.state = next_state\n",
    "        self.total_rewards += reward\n",
    "        return self.state, reward\n",
    "\n",
    "def run_episode(policy, n_steps, start_state=0, display=True):\n",
    "    env = MDPEnvironment()\n",
    "    if display:\n",
    "        print(\"States (+rewards):\", end=\" \")\n",
    "    for step in range(n_steps):\n",
    "        if display:\n",
    "            if step == 10:\n",
    "                print(\"...\", end=\" \")\n",
    "            elif step < 10:\n",
    "                print(env.state, end=\" \")\n",
    "        action = policy(env.state)\n",
    "        state, reward = env.step(action)\n",
    "        if display and step < 10:\n",
    "            if reward:\n",
    "                print(\"({})\".format(reward), end=\" \")\n",
    "    if display:\n",
    "        print(\"Total rewards =\", env.total_rewards)\n",
    "    return env.total_rewards\n",
    "\n",
    "for policy in (policy_fire, policy_random, policy_safe):\n",
    "    all_totals = []\n",
    "    print(policy.__name__)\n",
    "    for episode in range(1000):\n",
    "        all_totals.append(run_episode(policy, n_steps=100, display=(episode<5)))\n",
    "    print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-inf -inf -inf]\n",
      " [  0.   0. -inf]\n",
      " [-inf -inf -inf]]\n",
      "[[-inf -inf -inf]\n",
      " [-inf -inf -inf]\n",
      " [-inf -inf -inf]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.full((3, 3), -np.inf) # -inf for impossible actions\n",
    "Q[1, [0, 1]] = 0.0\n",
    "print(Q)\n",
    "Q = np.full((3, 3), -np.inf) # -inf for impossible actions\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 1, 2]\n",
      "1 [0, 2]\n",
      "2 [1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [  0., -inf,   0.],\n",
       "       [-inf,   0., -inf]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for state, actions in enumerate(possible_actions):\n",
    "    print(state, actions)\n",
    "    Q[state, actions] = 0.0 # Initial value = 0.0, for all possible actions\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "1 2\n",
      "2 1\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "discount_rate = 0.8\n",
    "n_iterations = 100\n",
    "for iteration in range(n_iterations):\n",
    "    Q_prev = Q.copy()\n",
    "    for s in range(3): #for each state\n",
    "        for a in possible_actions[s]: # for each action in state\n",
    "            if iteration/99 == 1:\n",
    "                print(s,a) \n",
    "            Q[s, a] = np.sum([T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp])) for sp in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 15.90909091,  12.72727273,  10.18181818],\n",
       "       [  0.        ,         -inf, -13.3201581 ],\n",
       "       [        -inf,  45.84980237,         -inf]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
