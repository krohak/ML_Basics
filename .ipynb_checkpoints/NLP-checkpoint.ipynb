{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "Deals with analyzing, understanding, and deriving information from the text data in a smart and efficient manner.\n",
    "\n",
    "\n",
    "Automatic Summarization, Machine Translation, Named Entity Recognition, Relationship Extraction, Sentiment Analysis, Speech Recognition, Topic Segmentation etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample text'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "_remove_noise(\"this is a sample text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multipli'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer \n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\" \n",
    "stem.stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiply'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "word = \"multiplying\" \n",
    "lem.lemmatize(word, \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech (pos) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('FANO', 'NNP'), ('Labs', 'NNP'), ('is', 'VBZ'), ('awesome', 'JJ'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "text = \"FANO Labs is awesome!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency – Inverse Document Frequency (TF – IDF)\n",
    "\n",
    "The model creates a vocabulary dictionary and assigns an index to each word. Each row in the output contains a tuple (i,j) and a tf-idf value of word at index j in document i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.58448290102\n",
      "  (0, 2)\t0.58448290102\n",
      "  (0, 4)\t0.444514311537\n",
      "  (0, 1)\t0.345205016865\n",
      "  (1, 1)\t0.385371627466\n",
      "  (1, 0)\t0.652490884513\n",
      "  (1, 3)\t0.652490884513\n",
      "  (2, 4)\t0.444514311537\n",
      "  (2, 1)\t0.345205016865\n",
      "  (2, 6)\t0.58448290102\n",
      "  (2, 5)\t0.58448290102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding (text vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2Vec model is composed of preprocessing module, a shallow neural network model called Continuous Bag of Words and another shallow neural network model called skip-gram. These models are widely used for all other nlp problems. It first constructs a vocabulary from the training corpus and then learns word embedding representations. Following code using gensim package prepares the word embedding as the vectors.\n",
    "\n",
    "They can be used as feature vectors for ML model, used to measure text similarity using cosine similarity techniques, words clustering and text classification techniques.\n",
    "\n",
    "See also: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.135327061591\n",
      "[ 0.00284716 -0.00244435 -0.00298516  0.0013916  -0.00094614  0.00126659\n",
      "  0.00277956 -0.00388456  0.00461074  0.0003646  -0.00472697 -0.00419046\n",
      " -0.00041253 -0.00245799  0.00374663  0.00022313  0.00261072 -0.00355409\n",
      "  0.00377335 -0.00278561  0.00313254 -0.00410708  0.00196203  0.00402066\n",
      " -0.00281726 -0.00068327  0.00210072 -0.00385393 -0.00310453 -0.00057065\n",
      "  0.00455011  0.00248685  0.00110605 -0.00466762  0.00039222  0.00476353\n",
      "  0.00277293  0.00323656 -0.00017605 -0.00301211 -0.002616   -0.00077974\n",
      " -0.00154974 -0.00197718  0.00218694  0.00284263 -0.00378764 -0.00401743\n",
      " -0.0041434  -0.00105117  0.00099916  0.00451826  0.00453743 -0.00183378\n",
      "  0.00043845  0.00342977  0.00047013  0.0043246   0.0026336   0.00305885\n",
      "  0.00136497 -0.00094814  0.00012126  0.00451916 -0.00469958  0.00289007\n",
      "  0.00095014 -0.00297767 -0.00263465 -0.00269361  0.0006098   0.00124558\n",
      " -0.0031856  -0.00194441 -0.00098953 -0.00429033 -0.00284154 -0.00325313\n",
      "  0.00375388 -0.00092285  0.0003386  -0.00132044  0.00249941 -0.00121859\n",
      " -0.00112371 -0.00077796 -0.00251418  0.00253893  0.00293304  0.00072263\n",
      "  0.00261578  0.00330533  0.00023665  0.00055784  0.00237848  0.00288714\n",
      "  0.00013723 -0.00247923 -0.00416133 -0.00091157]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krohak/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "/home/krohak/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['nlp', 'speech'], ['FANO', 'Labs', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print(model.similarity('data', 'learning'))\n",
    "print(model['speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n",
      "Class_B\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "training_corpus = [\n",
    "                   ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "test_corpus = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n",
    "\n",
    "model = NBC(training_corpus) \n",
    "print(model.classify(\"Their codes are amazing.\"))\n",
    "print(model.classify(\"I don't like their computer.\"))\n",
    "print(model.accuracy(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine [SVM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Class_A       0.50      0.67      0.57         3\n",
      "    Class_B       0.50      0.33      0.40         3\n",
      "\n",
      "avg / total       0.50      0.50      0.49         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm \n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for row in training_corpus:\n",
    "    train_data.append(row[0])\n",
    "    train_labels.append(row[1])\n",
    "\n",
    "test_data = [] \n",
    "test_labels = [] \n",
    "for row in test_corpus:\n",
    "    test_data.append(row[0]) \n",
    "    test_labels.append(row[1])\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "model = svm.SVC(kernel='linear') \n",
    "model.fit(train_vectors, train_labels) \n",
    "prediction = model.predict(test_vectors)\n",
    "\n",
    "print (classification_report(test_labels, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein Distance \n",
    "\n",
    "The Levenshtein distance between two strings is defined as the ** minimum number of edits needed to transform one string into the other **, with the allowable edit operations being insertion, deletion, or substitution of a single character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def levenshtein(s1,s2): \n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1 \n",
    "    distances = range(len(s1) + 1) \n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistances = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1]) \n",
    "            else:\n",
    "                 newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1]))) \n",
    "        distances = newDistances \n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshtein(\"analyze\",\"analyse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phonetic Matching \n",
    "\n",
    "A Phonetic matching algorithm takes a keyword as input (person’s name, location name etc) and produces a character string that identifies a set of words that are (roughly) phonetically similar. It is very useful for searching large text corpuses, correcting spelling errors and matching relevant names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'FS', None]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fuzzy \n",
    "soundex = fuzzy.Soundex(4) \n",
    "dmeta = fuzzy.DMetaphone()\n",
    "dmeta('fuzzy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity \n",
    "\n",
    "When the text is represented as vector notation, a general cosine similarity can also be applied in order to measure vectorized similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8485281374238569\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    common = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in common])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()]) \n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()]) \n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "   \n",
    "    if not denominator:\n",
    "        return 0.0 \n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text): \n",
    "    words = text.split() \n",
    "    return Counter(words)\n",
    "\n",
    "text1 = 'she sells sea shells on the sea shore' \n",
    "text2 = 'sea she on shells sells'\n",
    "\n",
    "vector1 = text_to_vector(text1) \n",
    "vector2 = text_to_vector(text2) \n",
    "cosine = get_cosine(vector1, vector2)\n",
    "print(cosine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
